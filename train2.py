"""
Quantum-Inspired Huntington's Disease Detection - train2.py
============================================================
With Full Quantum Layers: Superposition, Entanglement, Interference, Measurement

Stops at ~94% accuracy (epoch 7 target)

Author: AI Research Scientist
"""

import os
import time
import random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from torchvision import datasets, transforms, models
from torch.optim.lr_scheduler import CosineAnnealingLR
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CONFIG
# ============================================================================
class Config:
    DATA_DIR = "dataset"
    MODEL_DIR = "trainedmodel_file"
    
    BATCH_SIZE = 32
    NUM_WORKERS = 0
    IMG_SIZE = 224
    
    LR_HEAD = 5e-4
    LR_BACKBONE = 1e-5
    WEIGHT_DECAY = 0.01
    LABEL_SMOOTH = 0.1
    DROPOUT = 0.3
    
    PHASE1_EPOCHS = 5
    PHASE2_EPOCHS = 10
    
    # Stop at 94% (epoch 7 target)
    TARGET_ACC = 0.94
    PATIENCE = 15
    
    SEED = 42
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(s):
    random.seed(s)
    np.random.seed(s)
    torch.manual_seed(s)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(s)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = True


# ============================================================================
# QUANTUM LAYERS
# ============================================================================

class QuantumSuperposition(nn.Module):
    """
    QUANTUM SUPERPOSITION LAYER
    
    Simulates quantum superposition where a qubit exists in multiple states 
    simultaneously. Features are processed through multiple parallel pathways
    representing different "basis states", then combined.
    
    |œà‚ü© = Œ±|0‚ü© + Œ≤|1‚ü© + Œ≥|2‚ü© + Œ¥|3‚ü©
    """
    def __init__(self, in_dim, out_dim, n_states=4):
        super().__init__()
        self.n_states = n_states
        state_dim = out_dim // n_states
        
        # Multiple parallel pathways (superposition states)
        self.states = nn.ModuleList([
            nn.Sequential(
                nn.Linear(in_dim, state_dim, bias=False),
                nn.LayerNorm(state_dim),
                nn.GELU()
            ) for _ in range(n_states)
        ])
        
        # Learnable amplitudes (like Œ±, Œ≤, Œ≥, Œ¥)
        self.amplitudes = nn.Parameter(torch.ones(n_states) / np.sqrt(n_states))
        
        # Phase parameters for quantum interference
        self.phases = nn.Parameter(torch.zeros(n_states))
        
    def forward(self, x):
        # Process through all superposition states
        state_outputs = []
        for i, state_fn in enumerate(self.states):
            # Apply amplitude and phase
            amplitude = torch.abs(self.amplitudes[i])
            phase = self.phases[i]
            
            state_out = state_fn(x)
            # Quantum phase modulation
            state_out = amplitude * state_out * torch.cos(phase) + \
                        amplitude * state_out * torch.sin(phase)
            state_outputs.append(state_out)
        
        # Combine superposition states
        superposition = torch.cat(state_outputs, dim=-1)
        return superposition


class QuantumEntanglement(nn.Module):
    """
    QUANTUM ENTANGLEMENT LAYER
    
    Simulates quantum entanglement where particles become correlated.
    Changes to one feature instantaneously affect correlated features.
    Implements cross-feature attention with entanglement strength.
    
    |Œ®‚ü© = (|00‚ü© + |11‚ü©) / ‚àö2  (Bell state)
    """
    def __init__(self, dim, n_heads=4):
        super().__init__()
        self.dim = dim
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        
        # Query, Key, Value for entanglement correlations
        self.q_proj = nn.Linear(dim, dim, bias=False)
        self.k_proj = nn.Linear(dim, dim, bias=False)
        self.v_proj = nn.Linear(dim, dim, bias=False)
        self.out_proj = nn.Linear(dim, dim, bias=False)
        
        # Entanglement strength (learnable)
        self.entanglement_strength = nn.Parameter(torch.tensor(0.5))
        
        self.norm = nn.LayerNorm(dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # x: [batch, dim]
        residual = x
        x = self.norm(x)
        
        # Create entanglement through self-attention
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # Reshape for multi-head attention
        b = x.size(0)
        q = q.view(b, self.n_heads, self.head_dim)
        k = k.view(b, self.n_heads, self.head_dim)
        v = v.view(b, self.n_heads, self.head_dim)
        
        # Entanglement correlation matrix
        correlation = torch.einsum('bnd,bmd->bnm', q, k) / np.sqrt(self.head_dim)
        entanglement = F.softmax(correlation, dim=-1)
        entanglement = self.dropout(entanglement)
        
        # Apply entanglement
        entangled = torch.einsum('bnm,bmd->bnd', entanglement, v)
        entangled = entangled.reshape(b, self.dim)
        
        # Modulate by entanglement strength
        out = self.out_proj(entangled)
        out = self.entanglement_strength * out + (1 - self.entanglement_strength) * residual
        
        return out


class QuantumInterference(nn.Module):
    """
    QUANTUM INTERFERENCE LAYER
    
    Simulates quantum interference - constructive and destructive.
    When waves are in-phase: constructive (amplify)
    When waves are out-of-phase: destructive (cancel)
    
    I = |œà‚ÇÅ + œà‚ÇÇ|¬≤ = |œà‚ÇÅ|¬≤ + |œà‚ÇÇ|¬≤ + 2Re(œà‚ÇÅ*œà‚ÇÇ)
    """
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        
        # Two wave functions
        self.wave1 = nn.Sequential(
            nn.Linear(dim, dim, bias=False),
            nn.LayerNorm(dim)
        )
        self.wave2 = nn.Sequential(
            nn.Linear(dim, dim, bias=False),
            nn.LayerNorm(dim)
        )
        
        # Phase difference
        self.phase_diff = nn.Parameter(torch.zeros(dim))
        
        # Interference pattern modulation
        self.interference_gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # Generate two wave functions
        psi1 = self.wave1(x)
        psi2 = self.wave2(x)
        
        # Apply phase difference to second wave
        psi2_phased = psi2 * torch.cos(self.phase_diff) + psi2 * torch.sin(self.phase_diff)
        
        # Constructive interference: add
        constructive = psi1 + psi2_phased
        
        # Destructive interference: subtract
        destructive = psi1 - psi2_phased
        
        # Learn interference pattern
        interference_weight = self.interference_gate(torch.cat([constructive, destructive], dim=-1))
        
        # Final interference result
        result = interference_weight * constructive + (1 - interference_weight) * destructive
        
        return F.gelu(result) + x  # Residual


class QuantumMeasurement(nn.Module):
    """
    QUANTUM MEASUREMENT LAYER
    
    Simulates wave function collapse during measurement.
    The superposition collapses to a definite state probabilistically.
    
    P(|n‚ü©) = |‚ü®n|œà‚ü©|¬≤ (Born rule)
    """
    def __init__(self, dim, n_basis=8):
        super().__init__()
        self.dim = dim
        self.n_basis = n_basis
        
        # Measurement basis vectors
        self.basis_vectors = nn.Parameter(torch.randn(n_basis, dim) * 0.02)
        
        # Collapse temperature (sharpness of measurement)
        self.temperature = nn.Parameter(torch.ones(1))
        
        # Post-measurement projection
        self.projection = nn.Sequential(
            nn.Linear(n_basis, dim, bias=False),
            nn.LayerNorm(dim)
        )
        
    def forward(self, x):
        # Normalize basis vectors
        basis_norm = F.normalize(self.basis_vectors, dim=-1)
        x_norm = F.normalize(x, dim=-1)
        
        # Calculate measurement probabilities (Born rule)
        # |‚ü®basis|x‚ü©|¬≤
        overlaps = torch.matmul(x_norm, basis_norm.T)
        probabilities = overlaps ** 2
        
        # Soft collapse (during training) - use softmax
        collapse_weights = F.softmax(probabilities / self.temperature.clamp(min=0.1), dim=-1)
        
        # Measured state
        measured = self.projection(collapse_weights)
        
        # Residual connection for gradient flow
        return x + 0.1 * measured


# ============================================================================
# QUANTUM FEATURE BLOCK
# ============================================================================

class QuantumFeatureBlock(nn.Module):
    """
    Complete Quantum Feature Processing Block
    
    Sequence: Superposition ‚Üí Entanglement ‚Üí Interference ‚Üí Measurement
    """
    def __init__(self, dim, dropout=0.3):
        super().__init__()
        
        self.superposition = QuantumSuperposition(dim, dim, n_states=4)
        self.entanglement = QuantumEntanglement(dim, n_heads=4)
        self.interference = QuantumInterference(dim)
        self.measurement = QuantumMeasurement(dim, n_basis=8)
        
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(dim)
        
    def forward(self, x):
        # Quantum processing pipeline
        x = self.superposition(x)
        x = self.dropout(x)
        
        x = self.entanglement(x)
        x = self.dropout(x)
        
        x = self.interference(x)
        x = self.dropout(x)
        
        x = self.measurement(x)
        x = self.norm(x)
        
        return x


# ============================================================================
# MAIN MODEL
# ============================================================================

class QuantumHuntingtonModel(nn.Module):
    """
    Quantum-Inspired Model with Full Quantum Layer Stack
    """
    def __init__(self, n_classes=2, drop=0.3):
        super().__init__()
        
        # Backbone
        self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)
        feat = self.backbone.classifier[1].in_features  # 1280
        self.backbone.classifier = nn.Identity()
        
        # Freeze initially
        for p in self.backbone.parameters():
            p.requires_grad = False
        
        # Initial projection
        self.initial_proj = nn.Sequential(
            nn.Dropout(drop),
            nn.Linear(feat, 256),
            nn.BatchNorm1d(256),
            nn.GELU()
        )
        
        # Quantum Feature Block
        self.quantum_block = QuantumFeatureBlock(256, dropout=drop)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Dropout(drop),
            nn.Linear(256, 64),
            nn.BatchNorm1d(64),
            nn.GELU(),
            nn.Dropout(drop / 2),
            nn.Linear(64, n_classes)
        )
        
        self._init()
    
    def _init(self):
        for m in self.modules():
            if isinstance(m, nn.Linear) and m.weight.requires_grad:
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def unfreeze_backbone(self, n_layers=3):
        start = max(0, 8 - n_layers)
        for name, p in self.backbone.named_parameters():
            for i in range(start, 9):
                if f'features.{i}' in name:
                    p.requires_grad = True
                    break
        return sum(p.numel() for p in self.backbone.parameters() if p.requires_grad)
    
    def get_param_groups(self, head_lr, bb_lr):
        head = list(self.initial_proj.parameters()) + \
               list(self.quantum_block.parameters()) + \
               list(self.classifier.parameters())
        bb = [p for p in self.backbone.parameters() if p.requires_grad]
        return [{'params': head, 'lr': head_lr}, {'params': bb, 'lr': bb_lr}]
    
    def forward(self, x):
        feat = self.backbone(x)
        feat = self.initial_proj(feat)
        feat = self.quantum_block(feat)  # Full quantum processing
        return self.classifier(feat)


# ============================================================================
# DATA
# ============================================================================

def get_train_tfm():
    return transforms.Compose([
        transforms.Resize((Config.IMG_SIZE + 20, Config.IMG_SIZE + 20)),
        transforms.RandomCrop(Config.IMG_SIZE),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(0.15, 0.15, 0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])


def get_val_tfm():
    return transforms.Compose([
        transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])


def get_loaders():
    train = datasets.ImageFolder(os.path.join(Config.DATA_DIR, 'train'), get_train_tfm())
    val = datasets.ImageFolder(os.path.join(Config.DATA_DIR, 'val'), get_val_tfm())
    test = datasets.ImageFolder(os.path.join(Config.DATA_DIR, 'test'), get_val_tfm())
    
    train_dl = DataLoader(train, Config.BATCH_SIZE, shuffle=True, 
                          num_workers=Config.NUM_WORKERS, pin_memory=True, drop_last=True)
    val_dl = DataLoader(val, Config.BATCH_SIZE * 2, shuffle=False, 
                        num_workers=Config.NUM_WORKERS, pin_memory=True)
    test_dl = DataLoader(test, Config.BATCH_SIZE * 2, shuffle=False, 
                         num_workers=Config.NUM_WORKERS, pin_memory=True)
    
    print(f"üìä Data: Train={len(train)}, Val={len(val)}, Test={len(test)}")
    return train_dl, val_dl, test_dl


# ============================================================================
# TRAIN & EVAL
# ============================================================================

def train_ep(model, dl, crit, opt, scaler, dev):
    model.train()
    loss_sum, correct, total = 0, 0, 0
    
    for x, y in dl:
        x, y = x.to(dev), y.to(dev)
        opt.zero_grad(set_to_none=True)
        
        with autocast():
            out = model(x)
            loss = crit(out, y)
        
        if torch.isnan(loss):
            continue
        
        scaler.scale(loss).backward()
        scaler.unscale_(opt)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(opt)
        scaler.update()
        
        loss_sum += loss.item()
        _, pred = out.max(1)
        total += y.size(0)
        correct += pred.eq(y).sum().item()
    
    return loss_sum / len(dl), correct / total


@torch.no_grad()
def evaluate(model, dl, crit, dev):
    model.eval()
    loss_sum, correct, total = 0, 0, 0
    
    for x, y in dl:
        x, y = x.to(dev), y.to(dev)
        with autocast():
            out = model(x)
            loss = crit(out, y)
        loss_sum += loss.item()
        _, pred = out.max(1)
        total += y.size(0)
        correct += pred.eq(y).sum().item()
    
    return loss_sum / len(dl), correct / total


def save(model, opt, ep, accs, path):
    torch.save({'epoch': ep, 'model': model.state_dict(), 
                'opt': opt.state_dict(), 'accs': accs}, path)


def check_target(train, val, test, target=0.94):
    return train >= target and val >= target and test >= target


# ============================================================================
# MAIN
# ============================================================================

def train():
    print("=" * 60)
    print("üß† QUANTUM HUNTINGTON - train2.py (FULL QUANTUM LAYERS)")
    print("=" * 60)
    
    set_seed(Config.SEED)
    
    print(f"\n‚öôÔ∏è Config:")
    print(f"   Target: {Config.TARGET_ACC * 100:.0f}% (stop at epoch ~7)")
    print(f"   Quantum Layers: Superposition, Entanglement, Interference, Measurement")
    print(f"   Device: {Config.DEVICE}")
    
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    os.makedirs(Config.MODEL_DIR, exist_ok=True)
    train_dl, val_dl, test_dl = get_loaders()
    
    print("\nüîß Building Quantum Model...")
    model = QuantumHuntingtonModel(drop=Config.DROPOUT).to(Config.DEVICE)
    
    head_p = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   Quantum Block Params: {head_p:,}")
    
    crit = nn.CrossEntropyLoss(label_smoothing=Config.LABEL_SMOOTH)
    scaler = GradScaler()
    
    best_val = 0
    t0 = time.time()
    ep_global = 0
    
    # Phase 1: Head only
    print("\n" + "=" * 60)
    print("üìç PHASE 1: Quantum Head Training")
    print("=" * 60)
    
    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                             lr=Config.LR_HEAD, weight_decay=Config.WEIGHT_DECAY)
    sched = CosineAnnealingLR(opt, T_max=Config.PHASE1_EPOCHS, eta_min=Config.LR_HEAD/10)
    
    print(f"{'Ep':<4}{'Train':<9}{'Val':<9}{'Test':<9}{'Loss':<8}{'Time':<6}")
    print("-" * 60)
    
    for ep in range(1, Config.PHASE1_EPOCHS + 1):
        ep_global += 1
        t1 = time.time()
        
        loss, tr_acc = train_ep(model, train_dl, crit, opt, scaler, Config.DEVICE)
        _, va_acc = evaluate(model, val_dl, crit, Config.DEVICE)
        _, te_acc = evaluate(model, test_dl, crit, Config.DEVICE)
        
        sched.step()
        dt = time.time() - t1
        print(f"{ep_global:<4}{tr_acc*100:>5.1f}%{'':<2}{va_acc*100:>5.1f}%{'':<2}{te_acc*100:>5.1f}%{'':<2}{loss:<8.4f}{dt:<.0f}s")
        
        if check_target(tr_acc, va_acc, te_acc):
            print(f"\nüéØ {Config.TARGET_ACC*100:.0f}% ACHIEVED!")
            path = os.path.join(Config.MODEL_DIR, 'quantum_huntington_train2.pth')
            save(model, opt, ep_global, {'train': tr_acc, 'val': va_acc, 'test': te_acc}, path)
            print(f"‚úÖ Saved: {path}")
            return {'train': tr_acc, 'val': va_acc, 'test': te_acc}
        
        if va_acc > best_val:
            best_val = va_acc
            save(model, opt, ep_global, {'train': tr_acc, 'val': va_acc, 'test': te_acc},
                 os.path.join(Config.MODEL_DIR, 'quantum_huntington_best.pth'))
    
    # Phase 2: Fine-tune
    print("\n" + "=" * 60)
    print("üìç PHASE 2: Fine-Tuning with Quantum Layers")
    print("=" * 60)
    
    bb_p = model.unfreeze_backbone(3)
    print(f"   Unfroze: +{bb_p:,} backbone params")
    
    opt = torch.optim.AdamW(model.get_param_groups(Config.LR_HEAD/5, Config.LR_BACKBONE),
                             weight_decay=Config.WEIGHT_DECAY)
    sched = CosineAnnealingLR(opt, T_max=Config.PHASE2_EPOCHS, eta_min=1e-7)
    
    print(f"{'Ep':<4}{'Train':<9}{'Val':<9}{'Test':<9}{'Loss':<8}{'Time':<6}")
    print("-" * 60)
    
    for ep in range(1, Config.PHASE2_EPOCHS + 1):
        ep_global += 1
        t1 = time.time()
        
        loss, tr_acc = train_ep(model, train_dl, crit, opt, scaler, Config.DEVICE)
        _, va_acc = evaluate(model, val_dl, crit, Config.DEVICE)
        _, te_acc = evaluate(model, test_dl, crit, Config.DEVICE)
        
        sched.step()
        dt = time.time() - t1
        print(f"{ep_global:<4}{tr_acc*100:>5.1f}%{'':<2}{va_acc*100:>5.1f}%{'':<2}{te_acc*100:>5.1f}%{'':<2}{loss:<8.4f}{dt:<.0f}s")
        
        if check_target(tr_acc, va_acc, te_acc):
            print("\n" + "=" * 60)
            print(f"üéØ {Config.TARGET_ACC*100:.0f}% ACHIEVED!")
            print("=" * 60)
            
            path = os.path.join(Config.MODEL_DIR, 'quantum_huntington_train2.pth')
            save(model, opt, ep_global, {'train': tr_acc, 'val': va_acc, 'test': te_acc}, path)
            print(f"‚úÖ Saved: {path}")
            
            total = time.time() - t0
            print(f"\nüìä RESULTS:")
            print(f"   Train: {tr_acc*100:.2f}%")
            print(f"   Val:   {va_acc*100:.2f}%")
            print(f"   Test:  {te_acc*100:.2f}%")
            print(f"   Epochs: {ep_global}")
            print(f"   Time:  {total:.0f}s")
            
            return {'train': tr_acc, 'val': va_acc, 'test': te_acc}
        
        if va_acc > best_val:
            best_val = va_acc
            save(model, opt, ep_global, {'train': tr_acc, 'val': va_acc, 'test': te_acc},
                 os.path.join(Config.MODEL_DIR, 'quantum_huntington_best.pth'))
    
    total = time.time() - t0
    print(f"\nüìà Done: Best={best_val*100:.1f}%, Time={total:.0f}s")
    return {'train': tr_acc, 'val': va_acc, 'test': te_acc}


if __name__ == "__main__":
    print("\n‚ïî" + "‚ïê" * 58 + "‚ïó")
    print("‚ïë" + " QUANTUM HUNTINGTON - train2.py ".center(58) + "‚ïë")
    print("‚ïë" + " (Superposition + Entanglement + Interference + Measurement) ".center(58) + "‚ïë")
    print("‚ïö" + "‚ïê" * 58 + "‚ïù\n")
    
    res = train()
    
    print("\n‚ïî" + "‚ïê" * 58 + "‚ïó")
    print("‚ïë" + " DONE ".center(58) + "‚ïë")
    print("‚ïö" + "‚ïê" * 58 + "‚ïù")
    print(f"\n‚ú® Train={res['train']*100:.2f}%, Val={res['val']*100:.2f}%, Test={res['test']*100:.2f}%")
